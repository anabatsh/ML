{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9jcmQQo0Exe"
   },
   "source": [
    "#### Подключение библиотек\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o5nwCC5Mfds3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anabatsh/.local/lib/python3.8/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 7.1.0. Several security issues (CVE-2020-11538, CVE-2020-10379, CVE-2020-10994, CVE-2020-10177) have been fixed in pillow 7.1.0 or higher. We recommend to upgrade this library.\n",
      "  from .collection import imread_collection_wrapper\n"
     ]
    }
   ],
   "source": [
    "#IMPORT LIBS \n",
    "\n",
    "#download\n",
    "import glob\n",
    "import natsort\n",
    "import zipfile\n",
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "#numpy\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import numpy.testing as npt\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#skimage\n",
    "#!pip install scikit-image\n",
    "from skimage import data, transform, filters, io, color, util\n",
    "from skimage.util import img_as_float, img_as_ubyte\n",
    "from skimage.transform import AffineTransform, warp, rescale, rotate, resize\n",
    "from skimage.io import imread, imsave, imread_collection\n",
    "from skimage.filters import farid, sobel, threshold_li, gaussian, median\n",
    "from skimage.morphology import disk, binary_dilation, binary_erosion, area_opening, binary_closing\n",
    "from skimage.feature import register_translation\n",
    "\n",
    "#scipy\n",
    "from scipy.ndimage import fourier_shift\n",
    "\n",
    "#parallel\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#display\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import colorsys\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDajOZxo23t0"
   },
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ToTensor():\n",
    "    def __init__(self):\n",
    "        self.tr = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tr(img)\n",
    "\n",
    "class DeNoise():\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.to(device)\n",
    "        fft = torch.fft.rfft2(img)\n",
    "        fft = torch.fft.irfft2(fft)\n",
    "        return fft\n",
    "\n",
    "class ToDevice():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.to(self.device)\n",
    "        return img\n",
    "\n",
    "class Normalize():\n",
    "    def __call__(self, img):\n",
    "        max = torch.max(img)\n",
    "        min = torch.min(img)\n",
    "        mean = (max - min) / 2\n",
    "        std = max - mean\n",
    "\n",
    "        img = (img - mean) / std\n",
    "        img = (img * 127.5 + 127.5)\n",
    "        return img\n",
    "\n",
    "class Compose():\n",
    "    def __init__(self, *args):\n",
    "        self.compose = transforms.Compose(*args)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.compose(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torch.nn.functional import conv2d, conv_transpose2d\n",
    "\n",
    "def find_pixel_shift(base_img, move_img, conv_kernel, rad=1):\n",
    "    best_score, results = None, (0, 0)\n",
    "    \n",
    "    base_h, base_w = base_img.shape[-2:]\n",
    "    move_h, move_w = move_img.shape[-2:]\n",
    "    h = max(base_h, move_h)\n",
    "    w  = max(base_w, move_w)\n",
    "    \n",
    "    THRESHOLD = 100\n",
    "    if h > THRESHOLD or w > THRESHOLD:\n",
    "        (tmp_i, tmp_j), rad = find_pixel_shift(conv2d(base_img, conv_kernel, stride=3), \n",
    "                                               conv2d(move_img, conv_kernel, stride=3), \n",
    "                                               conv_kernel, rad)\n",
    "        \n",
    "        tmp_i *= 3; tmp_j *= 3;\n",
    "        \n",
    "        l_i, l_j = tmp_i-1, tmp_j-1\n",
    "        r_i, r_j = tmp_i+1, tmp_j+1\n",
    "        \n",
    "    else:\n",
    "        l_i, l_j = -rad, -rad\n",
    "        r_i, r_j = rad, rad\n",
    "        \n",
    "    rad += 1\n",
    "#     print(rad, l_i, r_i+1, l_j, r_j+1)\n",
    "    for i in range(l_i, r_i + 1):\n",
    "        for j in range(l_j, r_j + 1):\n",
    "            \n",
    "            a, b, c, d = rad+i, -rad+i, rad+j, -rad+j \n",
    "            b = b if b else h\n",
    "            d = d if d else w\n",
    "            MSE_score = torch.mean(torch.square(\n",
    "                base_img[0][0][rad:-rad, rad:-rad] -\n",
    "                move_img[0][0][a:b, c:d]\n",
    "            ))\n",
    "            if best_score == None or best_score > MSE_score:\n",
    "                best_score = MSE_score\n",
    "                results = (i, j)\n",
    "    return results, 3 * (rad - 1)\n",
    "\n",
    "def find_subpixel_shift(base_img, move_img, conv_kernel):\n",
    "    \n",
    "    (y, x), rad = find_pixel_shift(base_img, move_img, conv_kernel)\n",
    "    \n",
    "    h, w = base_img.shape[-2:]\n",
    "    res = torch.empty(5, device=device)\n",
    "    rad = rad // 3 + 1\n",
    "#     print(f'shift: {y, x}, rad: {rad}')\n",
    "    for res_ind, (i, j) in enumerate(((y-1, x), (y, x-1), (y, x), (y, x+1), (y+1, x))):\n",
    "        a, b, c, d = rad+i, -rad+i, rad+j, -rad+j \n",
    "        b = b if b else h\n",
    "        d = d if d else w\n",
    "#         print(f'i: {i}, j: {j}, a, b, c, d: {a, b, c, d}')\n",
    "        res[res_ind] = torch.mean(torch.square(\n",
    "            base_img[0][0][rad:-rad, rad:-rad] -\n",
    "            move_img[0][0][a:b, c:d]\n",
    "        ))\n",
    "    u, l, c, r, d = res\n",
    "    dy = (d - u) / (2 * (d + u - 2 * c))\n",
    "    dx = (r - l) / (2 * (l + r - 2 * c))\n",
    "    \n",
    "    return y-dy.item(), x-dx.item()\n",
    "\n",
    "def pixel_shift(img, shift):\n",
    "    y, x = shift\n",
    "    h, w = img.shape[-2:]\n",
    "    \n",
    "    if y != 0:\n",
    "        a, b, c, d, e, f = (0, -y, y, h, -y, h) if y > 0 else (-y, h, 0, y, 0, -y)\n",
    "        img[..., a:b, 0:w] = img[..., c:d, 0:w].detach().clone()\n",
    "        print\n",
    "        img[..., e:f, 0:w] = 0\n",
    "\n",
    "    if x != 0:\n",
    "        a, b, c, d, e, f = (0, -x, x, w, -x, w) if x > 0 else (-x, w, 0, x, 0, -x)\n",
    "        img[..., 0:h, a:b] = img[..., 0:h, c:d].detach().clone()\n",
    "        img[..., 0:h, e:f] = 0\n",
    "\n",
    "    return img\n",
    "            \n",
    "def subpixel_shift(img, d_shift):\n",
    "    dy, dx = d_shift\n",
    "    c_dy, c_dx = abs(dy), abs(dx)\n",
    "    h, w = img.shape[-2:]\n",
    "    \n",
    "    if dy != 0.:\n",
    "        a, b, c, d = (0, -1, 1, h) if dy > 0. else (1, h, 0, -1)\n",
    "        S = img[..., a:b, 0:w] * (1 - c_dy)\n",
    "        S = S + img[..., c:d, 0:w] * c_dy\n",
    "        img[..., a:b, 0:w] = S\n",
    "        \n",
    "    if dx != 0.:\n",
    "        a, b, c, d = (0, -1, 1, w) if dx > 0. else (1, w, 0, -1)       \n",
    "        S = img[..., 0:h, a:b] * (1 - c_dx)\n",
    "        S = S + img[..., 0:h, c:d] * c_dx\n",
    "        img[..., 0:h, a:b] = S\n",
    "\n",
    "    return img\n",
    "\n",
    "def special_subpixel_shift(img, shift):    \n",
    "    y_dy, x_dx = shift\n",
    "    y, x = int(y_dy), int(x_dx)\n",
    "    dy, dx = y_dy - y, x_dx - x\n",
    "    \n",
    "    img = subpixel_shift(img, (dy, dx))\n",
    "    img = pixel_shift(img, (y, x))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvStab_Operator():\n",
    "    \n",
    "    def __init__(self, base_img, steps=3):\n",
    "        \n",
    "        self.base_img = base_img\n",
    "\n",
    "        self.steps = steps + 1\n",
    "        self.kernel = torch.ones(1,1,3,3).to(device) * 1/9\n",
    "        self.base_conv = self.convolve(self.base_img)\n",
    "    \n",
    "    def convolve(self, img):\n",
    "        conv = []\n",
    "        \n",
    "        for step in range(self.steps):\n",
    "            if step:\n",
    "                img = conv2d(img, self.kernel, stride=3)\n",
    "            conv.append(img)\n",
    "            \n",
    "        return conv\n",
    "    \n",
    "    \n",
    "    def stabilize(self, img):\n",
    "        \n",
    "        move_conv = self.convolve(img)\n",
    "        \n",
    "        best_score = None\n",
    "        result = (0, 0)\n",
    "        \n",
    "        for step in range(self.steps)[::-1]:\n",
    "\n",
    "            rad = 3 ** (3 - step) + 1\n",
    "            \n",
    "            res_i, res_j = result\n",
    "            \n",
    "            for i in range(res_i-1, res_j+2):\n",
    "                for j in range(res_i-1, res_j+2):\n",
    "\n",
    "                    a, b, c, d = rad+i, -rad+i, rad+j, -rad+j \n",
    "                    \n",
    "                    MSE_score = torch.mean(torch.square(\n",
    "                        self.base_conv[step][0][0][rad:-rad, rad:-rad] -\n",
    "                        move_conv[step][0][0][a:b, c:d]\n",
    "                    ))\n",
    "                    \n",
    "#                     print(f'({rad}), s: {i, j} score: {MSE_score.item():.3f}')\n",
    "                    \n",
    "                    if best_score == None or best_score > MSE_score:\n",
    "                        best_score = MSE_score\n",
    "                        result = (i*3, j*3) if step else (i, j)\n",
    "                        \n",
    "        corr = [0] * 5\n",
    "        y, x = result\n",
    "        \n",
    "#         for idx, (i, j) in enumerate(((y-1, x), (y, x-1), (y, x), (y, x+1), (y+1, x))):\n",
    "#             a, b, c, d = rad+i, -rad+i, rad+j, -rad+j \n",
    "\n",
    "#             corr[idx] = torch.mean(torch.square(\n",
    "#                 self.base_conv[step][0][0][rad:-rad, rad:-rad] -\n",
    "#                 move_conv[step][0][0][a:b, c:d]\n",
    "#             ))\n",
    "        \n",
    "#         u, l, c, r, d = corr\n",
    "#         dy = (d - u) / (2 * (d + u - 2 * c))\n",
    "#         dx = (r - l) / (2 * (l + r - 2 * c))\n",
    "    \n",
    "#         y, x = y-dy.item(), x-dx.item()\n",
    "#         print(y, x)\n",
    "        pad_ = [0] * 4\n",
    "        if y > 0:\n",
    "            pad_[3] = y\n",
    "        if y < 0:\n",
    "            pad_[2] = -y\n",
    "        if x > 0:\n",
    "            pad_[1] = x\n",
    "        if x < 0:\n",
    "            pad_[0] = -x\n",
    "        \n",
    "        img = torch.nn.functional.pad(img, pad_)\n",
    "        if y > 0:\n",
    "            img = img[..., y:, :]\n",
    "        if y < 0:\n",
    "            img = img[..., :y, :]\n",
    "            \n",
    "        if x > 0:\n",
    "            img = img[..., :, x:]\n",
    "        if x < 0:\n",
    "            img = img[..., :, :x]\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Видос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0612890256ca4e7c8aab58c06c4a0440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "#------Open the video\n",
    "\n",
    "video_name = 13\n",
    "\n",
    "cap = cv.VideoCapture(f'video/{video_name}.mp4')\n",
    "cap_width = int(cap.get(3))\n",
    "cap_height = int(cap.get(4))\n",
    "\n",
    "fourcc = cv.VideoWriter_fourcc('M','J','P','G')\n",
    "out = cv.VideoWriter(f'result/{video_name}.mp4', fourcc, 20.0, (cap_width, cap_height), isColor=False)\n",
    "\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "#------Read the video\n",
    "\n",
    "transform = Compose([ToTensor(), \n",
    "                     ToDevice(device),\n",
    "                     Normalize()\n",
    "                    ])\n",
    "\n",
    "cs = None\n",
    "\n",
    "def generator(val):\n",
    "    while val:\n",
    "        yield\n",
    "\n",
    "for i, _ in enumerate(tqdm(generator(cap.isOpened()))):\n",
    "# while(cap.isOpened()):\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        #------Main cycle\n",
    "        \n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        img = transform(img).unsqueeze(0)\n",
    "        \n",
    "        if cs is not None:\n",
    "            img = cs.stabilize(img)\n",
    "            pic_to_save = img.cpu()[0][0].numpy().astype(np.uint8)        \n",
    "            out.write(pic_to_save)\n",
    "#             cv.imshow('Frame', pic_to_save)\n",
    "            imsave(f'result/{i}.png', pic_to_save)\n",
    "\n",
    "        else:\n",
    "            cs = ConvStab_Operator(img, 3)\n",
    "\n",
    "        #------End cycle\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "        if i > 10:\n",
    "            break\n",
    "    else:\n",
    "        break  \n",
    "\n",
    "#------Close the video\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 1)\n",
      "(0, 1)\n",
      "(0, 1)\n",
      "(0, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(-4, 3)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (340) must match the size of tensor b (0) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-414-e414ebf6fe4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprev_img\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mshift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_pixel_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mcurr_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecial_subpixel_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-375-8035aad94bfd>\u001b[0m in \u001b[0;36mfind_pixel_shift\u001b[0;34m(base_img, move_img, conv_kernel, rad)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             MSE_score = torch.mean(torch.square(\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mbase_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrad\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mmove_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             ))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (340) must match the size of tensor b (0) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "#------Open the video\n",
    "\n",
    "video_name = 3\n",
    "\n",
    "cap = cv.VideoCapture(f'video/{video_name}.mp4')\n",
    "cap_width = int(cap.get(3))\n",
    "cap_height = int(cap.get(4))\n",
    "\n",
    "fourcc = cv.VideoWriter_fourcc('M','J','P','G')\n",
    "out = cv.VideoWriter(f'result/{video_name}.mp4', fourcc, 20.0, (cap_width, cap_height), isColor=True)\n",
    "\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "#------Read the video\n",
    "\n",
    "transform = Compose([ToTensor(), \n",
    "                     ToDevice(device),\n",
    "                     Normalize()\n",
    "                    ])\n",
    "prev_img = None\n",
    "mask_img = None\n",
    "mask = None\n",
    "conv_kernel = torch.ones(1,1,3,3).to(device) * 1/9\n",
    "\n",
    "i = 1\n",
    "while(cap.isOpened()):\n",
    "    ret, curr_img = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        #------Main cycle\n",
    "        \n",
    "        curr_img = cv.cvtColor(curr_img, cv.COLOR_BGR2GRAY)\n",
    "        curr_img = transform(curr_img).unsqueeze(0)\n",
    "        \n",
    "        if prev_img is not None:\n",
    "            shift, _ = find_pixel_shift(prev_img, curr_img, conv_kernel)\n",
    "            print(shift)\n",
    "            curr_img = special_subpixel_shift(curr_img, shift)\n",
    "            \n",
    "            vasc += abs(curr_img - mask_img) / 255\n",
    "            pic_to_save = abs(curr_img - mask_img) * (curr_img > 0.) * (mask_img > 0.)\n",
    "            \n",
    "            if i % 30 == 0:\n",
    "                mask = (vasc > torch.quantile(vasc, 0.8)).to(torch.float32)\n",
    "                mask = conv2d(mask, conv_kernel, stride=1, padding=1) == 1.\n",
    "                vasc[...] = 0\n",
    "\n",
    "            if mask is not None:\n",
    "                pic_to_save = curr_img[0].repeat(3, 1, 1)\n",
    "                pic_to_save[0] *= 0.6\n",
    "                pic_to_save[1] *= 0.6\n",
    "                pic_to_save[2][~mask[0][0]] *= 0.6\n",
    "                pic_to_save = pic_to_save.transpose(0, 1).transpose(1, 2).cpu().numpy().astype(np.uint8)        \n",
    "                out.write(pic_to_save)\n",
    "                cv.imshow('Frame', pic_to_save)\n",
    "\n",
    "                \n",
    "            mask_img = curr_img\n",
    "#             prev_img = curr_img\n",
    "    \n",
    "        else:\n",
    "            prev_img = curr_img\n",
    "            mask_img = prev_img\n",
    "            vasc = torch.zeros_like(prev_img)\n",
    "\n",
    "        #------End cycle\n",
    "        i += 1\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break  \n",
    "\n",
    "#------Close the video\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RwCpckUM7tyv",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de978d9fc1a4f0ea4ca7e6adaa7b4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "step:   0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prev_img = transform(images[0]).unsqueeze(0)\n",
    "mask_img = prev_img\n",
    "vasc = torch.zeros_like(prev_img)\n",
    "mask = None\n",
    "pics_to_save = []\n",
    "\n",
    "conv_kernel = torch.ones(1,1,3,3).to(device) * 1/9\n",
    "\n",
    "for i, curr_img in enumerate(tqdm(images[1:], desc='step')):\n",
    "    curr_img = transform(curr_img).unsqueeze(0)\n",
    "    shift = find_subpixel_shift(prev_img, curr_img, conv_kernel)\n",
    "    curr_img = special_subpixel_shift(curr_img, shift)\n",
    "    \n",
    "#     vasc += abs(curr_img - mask_img) / 255\n",
    "    pic_to_save = abs(curr_img - mask_img) * (curr_img > 0.) * (mask_img > 0.)\n",
    "#     if (i + 1) % 100 == 0:\n",
    "#         mask = (vasc > torch.quantile(vasc, 0.8)).to(torch.float32)\n",
    "#         mask = conv2d(mask, conv_kernel, stride=1, padding=1) == 1.\n",
    "# #         plt.figure(figsize=(10, 10))\n",
    "# #         plt.imshow(mask.cpu()[0][0])\n",
    "# #         plt.show()\n",
    "#         vasc[...] = 0\n",
    "\n",
    "        \n",
    "#     if mask is not None:\n",
    "#         pic_to_save = curr_img[0].repeat(3, 1, 1)\n",
    "#         pic_to_save[0] *= 0.2\n",
    "#         pic_to_save[1] *= 0.2\n",
    "#         pic_to_save[2][~mask[0][0]] *= 0.2\n",
    "#         pic_to_save = pic_to_save.transpose(0, 1).transpose(1, 2).cpu().numpy().astype(np.uint8)        \n",
    "#         pics_to_save.append(pic_to_save)\n",
    "\n",
    "#     pic_to_save = curr_img[0][0].cpu().numpy().astype(np.uint8)        \n",
    "    pics_to_save.append(pic_to_save[0][0].cpu().numpy().astype(np.uint8))\n",
    "    mask_img = curr_img\n",
    "\n",
    "#     prev_img = curr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open PIV again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_clarification(R, shift, device, up_c=100):\n",
    "\n",
    "    im2pi = 1j * 2 * torch.pi\n",
    "\n",
    "    up_c_part = int(up_c / 2)\n",
    "    dfshift = up_c_part - shift * up_c\n",
    "    shape = R.shape[1:]\n",
    "\n",
    "    for i in (1, 0):\n",
    "        kernel = torch.arange(1.5 * up_c).to(device)[:, None] - dfshift[i]\n",
    "        fft_req = torch.fft.fftfreq(shape[i], up_c, dtype=torch.float64, device=device)\n",
    "        kernel = kernel * fft_req\n",
    "        kernel = torch.exp(-im2pi * kernel).to(device)\n",
    "        R = torch.tensordot(kernel, R, dims=([1], [-1]))\n",
    "\n",
    "    R = R.transpose(2, 0).transpose(1, 2)\n",
    "    sub_shift =  unravel_ind(R.conj()).to(device) - up_c_part\n",
    "    sub_shift = sub_shift / up_c\n",
    "\n",
    "    return sub_shift\n",
    "\n",
    "\n",
    "def unravel_ind(corr):\n",
    "    w = corr.shape[-1]\n",
    "    max_ind = torch.argmax(torch.abs(corr))\n",
    "    shift = torch.tensor([max_ind // w, max_ind % w])\n",
    "    return shift\n",
    "\n",
    "def phase_cross_correlation(base_img, mov_img, device, up_c=100):\n",
    "\n",
    "    base_fft = torch.fft.fftn(base_img)\n",
    "    mov_fft = torch.fft.fftn(mov_img)\n",
    "\n",
    "    R = (base_fft * mov_fft.conj()).type(torch.complex128)\n",
    "    corr = torch.fft.ifftn(R)\n",
    "\n",
    "    shape = corr.shape[1:]\n",
    "    shape = torch.tensor(shape, device=device)\n",
    "    shift =  unravel_ind(corr).to(device)\n",
    "    midpoints = torch.fix(shape / 2)\n",
    "    shift[shift > midpoints] -= shape[shift > midpoints]\n",
    "\n",
    "    sub_shift = corr_clarification(R.conj(), shift, device, up_c=up_c)\n",
    "\n",
    "    shift = shift + sub_shift\n",
    "\n",
    "    return shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_shape(h, w, window_size, overlap):\n",
    "    \n",
    "    k = window_size - overlap\n",
    "    \n",
    "    f_h = (h - window_size) // k + 1\n",
    "    f_w = (w - window_size) // k + 1\n",
    "    \n",
    "    return f_h, f_w\n",
    "    \n",
    "def get_coordinates(h, w, window_size, overlap):\n",
    "    \n",
    "    k = window_size - overlap\n",
    "\n",
    "    f_h, f_w = get_field_shape(h, w, window_size, overlap)\n",
    "    \n",
    "    i = torch.arange(f_h)\n",
    "    j = torch.arange(f_w)\n",
    "    \n",
    "    y = i * k\n",
    "    x = j * k\n",
    "    \n",
    "    II, JJ = torch.meshgrid(i, j)\n",
    "    YY, XX = torch.meshgrid(y, x)\n",
    "    \n",
    "    IJYX = torch.stack((II, JJ, YY, XX), axis=-1).view(-1, 4)\n",
    "    return IJYX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(img, window_size, overlap):\n",
    "    h, w = img.shape[-2:]\n",
    "    s = window_size\n",
    "    return [img[..., x:x+s, y:y+s] for x, y in get_coordinates(h, w, window_size, overlap)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [imread(file) for file in glob.glob('result/*.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_a = transform(images[0]).unsqueeze(0)\n",
    "img_b = transform(images[1]).unsqueeze(0)\n",
    "\n",
    "h, w = img_b.shape[-2:]\n",
    "window_size = 11\n",
    "overlap = 5\n",
    "conv_kernel = torch.ones(1,1,3,3).to(device) * 1/9\n",
    "\n",
    "f_h, f_w = get_field_shape(h, w, window_size, overlap)\n",
    "field = torch.zeros(f_h, f_w, 2)\n",
    "\n",
    "for i, j, x, y in get_coordinates(h, w, window_size, overlap):\n",
    "    s = window_size\n",
    "    w_shift = find_subpixel_shift(img_a[..., x:x+s, y:y+s], \n",
    "                                  img_b[..., x:x+s, y:y+s], \n",
    "                                  conv_kernel=conv_kernel)\n",
    "    field[i][j] = torch.tensor(w_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag = torch.norm(field, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = torch.arctan(field[...,0] / field[...,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mag = torch.quantile(mag, 0.9)\n",
    "min_mag = torch.quantile(mag, 0.8)\n",
    "mag_ = torch.clip(mag, min_mag, max_mag)\n",
    "mag_bin = mag_ > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2dc4aa7f70>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(mag_bin * angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3496.7852)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def display_vector_field(field):\n",
    "    a = np.loadtxt(filename)\n",
    "    # parse\n",
    "    x, y, u, v, mask = a[:, 0], a[:, 1], a[:, 2], a[:, 3], a[:, 4]\n",
    "\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        fig = ax.get_figure()\n",
    "\n",
    "    if on_img is True:  # plot a background image\n",
    "        im = imread(image_name)\n",
    "        im = negative(im)  # plot negative of the image for more clarity\n",
    "        # imsave('neg.tif', im)\n",
    "        # im = imread('neg.tif')\n",
    "        xmax = np.amax(x) + window_size / (2 * scaling_factor)\n",
    "        ymax = np.amax(y) + window_size / (2 * scaling_factor)\n",
    "        ax.imshow(im, cmap=\"Greys_r\", extent=[0.0, xmax, 0.0, ymax])\n",
    "        # plt.draw()\n",
    "\n",
    "    invalid = mask.astype(\"bool\")  \n",
    "    valid = ~invalid\n",
    "\n",
    "\n",
    "    ax.quiver(\n",
    "        x[invalid], y[invalid], u[invalid], v[invalid], color=\"r\", width=width, **kw)\n",
    "    ax.quiver(x[valid], y[valid], u[valid], v[valid], color=\"b\", width=width,**kw)\n",
    "    \n",
    "    # if on_img is False:\n",
    "    #     ax.invert_yaxis()\n",
    "    \n",
    "    ax.set_aspect(1.)\n",
    "    # fig.canvas.set_window_title('Vector field, '+str(np.count_nonzero(invalid))+' wrong vectors')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"Matching.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3e6bea8ee5ca42bebb8841c3a3e08f75": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4352fae4136549929e84eb6f38bfc24d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a920fe1aad24b34a0050c21a1c82fad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e603a3487a74c3c807effc6760d197b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "step: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb8de7ca152f4970966c872b3c3ea078",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c017adbe5eb044d28d19b7718e82e984",
      "value": 1
     }
    },
    "5149113cf15a4c209abc0a660e3bc52c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "62db9d507a934bef8034f00a1a4e7946": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "step: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af1f2d8b54a946f58b8495fdfdbe1012",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5149113cf15a4c209abc0a660e3bc52c",
      "value": 29
     }
    },
    "6b187b2e91c2467c9313ef938024d343": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e603a3487a74c3c807effc6760d197b",
       "IPY_MODEL_e9fdbfce612d4597a603c7cf35d917e9"
      ],
      "layout": "IPY_MODEL_e32d90b29012402880f4690508720899"
     }
    },
    "71bd35f59959435ebe310c6c901661f7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af1f2d8b54a946f58b8495fdfdbe1012": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c017adbe5eb044d28d19b7718e82e984": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c83a5a70cd0449e8b12567115bbb78a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4352fae4136549929e84eb6f38bfc24d",
      "placeholder": "​",
      "style": "IPY_MODEL_4a920fe1aad24b34a0050c21a1c82fad",
      "value": " 29/29 [00:35&lt;00:00,  1.23s/it]"
     }
    },
    "cb8de7ca152f4970966c872b3c3ea078": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5f2ad74259f4a04be8a9681c0579a40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62db9d507a934bef8034f00a1a4e7946",
       "IPY_MODEL_c83a5a70cd0449e8b12567115bbb78a4"
      ],
      "layout": "IPY_MODEL_71bd35f59959435ebe310c6c901661f7"
     }
    },
    "e2b7c462367349628ffbd71d5727e64c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e32d90b29012402880f4690508720899": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9fdbfce612d4597a603c7cf35d917e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e6bea8ee5ca42bebb8841c3a3e08f75",
      "placeholder": "​",
      "style": "IPY_MODEL_e2b7c462367349628ffbd71d5727e64c",
      "value": " 1/1 [00:00&lt;00:00,  2.22it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
